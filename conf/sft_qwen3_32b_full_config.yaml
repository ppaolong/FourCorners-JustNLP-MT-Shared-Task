# SFT Training Configuration for Gemma-3 English-Hindi Translation

# Model Configuration
model:
  model_name: "unsloth/Qwen3-32b"
  max_seq_length: 2048  # Choose any for long context
  load_in_4bit: true  # 4 bit quantization to reduce memory
  load_in_8bit: false  # A bit more accurate, uses 2x memory
  full_finetuning: false  # Set to true for full finetuning

  # LoRA Configuration
  lora_rank: 256  # Larger = higher accuracy, but might overfit
  lora_alpha: 512  # Recommended alpha == r at least
  lora_dropout: 0
  bias: "none"
  finetune_attention_modules: true  # Attention good for translation
  finetune_mlp_modules: true  # Should leave on always

# Data Configuration
data:
  dataset_name: "JustNLP-MT"  # Hugging Face dataset name
  split: "train"  # Dataset split to use
  source_column: "Source"
  translation_column: "Translation"
  difficulty_column: "Difficulty"  # Column name for difficulty levels
  difficulty_levels: null
  max_samples: null  # Set to a number to limit samples, null for all

# Chat Template Configuration
template:
  chat_template: "qwen-3"
  system_prompt: |
    You are an expert English to Hindi legal translator. Translate the given Legal English text to Legal Hindi accurately and naturally. The structure of the sentence in Hindi should maintain the legal nuance.

# SFT Training Configuration
sft:
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1  # Use GA to mimic batch size
  warmup_ratio: 0.1
  num_train_epochs: 3  # Set this for 1 full training run
  learning_rate: 1.0e-4  # Reduce to 2e-5 for long training runs
  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  

# Output Configuration
output:
  save_lora_path: "outputs/sft_qwen3_32b_full"  # Local path to save LoRA adapters
  save_merged: true  # Set to true to also save merged model
  save_gguf: false

# Logging
logging:
  report_to: "wandb"  # Use "wandb" for WandB logging
  wandb_project: "JustNLP-MT"
  wandb_run_name: "sft_qwen3_32b_full"

# Seed
seed: 44